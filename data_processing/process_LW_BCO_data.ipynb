{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ab25ad2-14a7-4b82-b146-c1d4cb54a120",
   "metadata": {},
   "source": [
    "The aim of this notebook is to process downward longwave (LW_down) data and cloud mask radar data from the BCO. After creating files of measurements, this code aims at separating the LW_down from each cloud, to plot the composite image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66c96f12-27b2-423b-98da-20ccf8a78d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import intake\n",
    "import os\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask.array as da\n",
    "import pandas as pd\n",
    "import glob\n",
    "from dask.diagnostics import ProgressBar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98074e7-2718-487b-96ac-738282b8188f",
   "metadata": {},
   "source": [
    "## Get $LW\\downarrow$ data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870c2d41-1fd1-4615-b4ea-012d37288e17",
   "metadata": {},
   "source": [
    "The aim of this part is to create a xarray containing the meaaured $LW\\downarrow$ for the entire year of 2020. We use the data from the BCO to achieve this using the internal file storage on levante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa26a6e-8578-47d7-a447-d6afd2c94a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "fileformat = f'/pool/data/OBS/BARBADOS_CLOUD_OBSERVATORY/Level_1/K_Radiation/%Y%m/Radiation__Deebles_Point__DownwellingRadiation__1s__%Y%m%d.nc'\n",
    "dates = pd.date_range('2020-01-12','2020-11-30')\n",
    " \n",
    "def create_filelist(dates, fileformat):\n",
    "    \"\"\"\n",
    "    Create list of files\n",
    " \n",
    "    Returns list of existing files for\n",
    "    the given dates and fileformat\n",
    " \n",
    "    Input\n",
    "    -----\n",
    "    dates : datetime-like\n",
    "        List of dates that are of interest\n",
    "    fileformat : str\n",
    "        Fileformat of files incl. full path\n",
    " \n",
    "    Returns\n",
    "    -------\n",
    "    files : list\n",
    "        List of files that could be found\n",
    "    \"\"\"\n",
    "    files = []\n",
    "    for date in dates:\n",
    "        fn = date.strftime(fileformat)\n",
    "        if os.path.exists(fn):\n",
    "            files.append(fn)\n",
    "    return files\n",
    " \n",
    " \n",
    "files_available = create_filelist(dates, fileformat)\n",
    "def preprocess_remove_duplicates(ds):\n",
    "    _, index = np.unique(ds['time'], return_index=True)\n",
    "    return ds.isel(time=index)\n",
    "\n",
    "# Use preprocess function to remove duplicates\n",
    "ds_radiation = xr.open_mfdataset(files_available, preprocess=preprocess_remove_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "663cc6ea-7e71-4413-bc82-831e566803a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "LW_down = ds_radiation.LWdown_diffuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc198429-8de2-4cd4-b699-52867564f950",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We also get the windspeed data from 2020, and do an average at 1000m. This average is used as an approximation to get the mean windspeed.\n",
    "This value is used to convert time to distances.\n",
    "\"\"\"\n",
    "windspeed = xr.open_dataset('./../data/windspeed_eurec4a.nc')['wind_speed']\n",
    "windspeed_avg = windspeed.sel(range = 1000, method = 'nearest').mean(dim = 'time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd5870d-9ce4-4f70-8569-81b6153ac2eb",
   "metadata": {},
   "source": [
    "## Get cloud mask data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd822dd0-2a9a-4542-be0a-cec7ede89a3d",
   "metadata": {},
   "source": [
    "In this part of the code, we aim at processing the cloud mask data from the BCO. The idea is to create an array with all the measured clouds. We include the information on the clouds: start and end time, lenght, depth, bottom and top height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0d4c24-6ec3-44c9-b06d-4619f995e6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cloud_borders(cloud_mask, min_cloud_length, max_cloud_length):\n",
    "    \"\"\"\n",
    "    Create an array with all observed clouds with their information\n",
    " \n",
    "    Input\n",
    "    -----\n",
    "    cloud_mask : cloud mask product from the BCO radar measurement\n",
    "    min_cloud_length, max_cloud_length: minimum and maximum cloud lengths allowed\n",
    " \n",
    "    Returns\n",
    "    -------\n",
    "    arrays of observed clouds with their informations (characteristics)\n",
    "    \"\"\"\n",
    "    cloud_times = []\n",
    "    \n",
    "    for cloud_index in cloud_mask.N_clouds:\n",
    "        \n",
    "        cloud_start_time = np.datetime64(cloud_mask.cloudStartTime[int(cloud_index-1)].values)\n",
    "        \n",
    "        if start_time <= cloud_start_time <= end_time:\n",
    "            cloud_length = cloud_mask.cloudLength[int(cloud_index-1)].values\n",
    "            \n",
    "            if  max_cloud_length >= cloud_length >= min_cloud_length:\n",
    "                \n",
    "                cloud_end_time = cloud_mask.cloudEndTime[int(cloud_index-1)].values\n",
    "                cloud_base_height = cloud_mask.cloudBase[int(cloud_index-1)].values\n",
    "                cloud_top_height = cloud_mask.cloudTop[int(cloud_index-1)].values\n",
    "                cloud_mid_height = (cloud_top_height - cloud_base_height)/2 + cloud_base_height\n",
    "                cloud_times.append((cloud_start_time, cloud_end_time, cloud_length, cloud_base_height, \n",
    "                                    cloud_top_height, cloud_mid_height))\n",
    "\n",
    "    cloud_times_array = np.array(cloud_times)\n",
    "    \n",
    "    return cloud_times_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d630891-5b07-4974-9003-61811b7b5c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = np.datetime64(dates[0].to_pydatetime())\n",
    "end_time = np.datetime64(dates[-1].to_pydatetime())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7c18ed-534c-4279-a06d-262baeb9bbc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cloud mask data from the BCO radar:\n",
    "data_cloud_mask = xr.open_dataset('./../data/cloudObjectMask_MBR_155m-18km_200112-201130_v0.3.nc')\n",
    "\n",
    "#we filter clouds also with length, to avoid to have clouds bigger than 50km. Clouds lenghts and depth will be further filtered later\n",
    "min_cloud_length = 0\n",
    "max_cloud_length = 50000\n",
    "clouds = cloud_borders(data_cloud_mask, min_cloud_length, max_cloud_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5285e893-bd10-42cf-8fc9-acdd21351af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('./../data/processed_data/cloud_borders_2020.txt', clouds, delimiter=',', fmt='%s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5393a607-272f-40e5-8ee2-7b27b900f656",
   "metadata": {},
   "outputs": [],
   "source": [
    "clouds = np.loadtxt('./../data/processed_data/cloud_borders_2020.txt', delimiter=',', dtype='object') \n",
    "clouds[:, 0] = np.array(clouds[:, 0], dtype='datetime64[s]')\n",
    "clouds[:, 1] = np.array(clouds[:, 1], dtype='datetime64[s]')\n",
    "clouds[:, 2] = np.array(clouds[:, 2], dtype='float64')\n",
    "clouds[:, 3] = np.array(clouds[:, 3], dtype='float64')\n",
    "clouds[:, 4] = np.array(clouds[:, 4], dtype='float64')\n",
    "clouds[:, 5] = np.array(clouds[:, 5], dtype='float64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b770194-f450-4702-b79b-197cacb1425d",
   "metadata": {},
   "source": [
    "## Combine cloud data and $LW\\downarrow$ measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a044e31-d9b5-4690-b95e-7211ee33a066",
   "metadata": {},
   "source": [
    "Now that we have $LW\\downarrow$ data and an array with clouds and their characteristics, we can isolate the emission signal from 60min before and after the clouds. This gives us an xarray of measurements over time, with one dimension per cloud. The coordinate is still time\n",
    "\n",
    "This will then be used to plot a composite of the average $LW\\downarrow$ signal for all clouds.\n",
    "\n",
    "However to avoid cross contamination in the signal (in each cloud $LW\\downarrow$ array, there might be other clouds closer than 60min), therefore, we set to NaN the values of each cloud. This is justified as we are interested in the surroundings of clouds and not in clouds themselves\n",
    "\n",
    "To summarise, here are the processing steps:\n",
    "1) Set to NaN all the in-cloud values to avoid cross-contamination, the resulting xarray is called LW_down_Nan;\n",
    "2) Separate the $LW\\downarrow$ signal into single clouds arrays using the cloud data and the overall $LW\\downarrow$ measurements;\n",
    "3) Convert the time coordinate into a distance coordinate (to better analyse haloes).\n",
    "4) Save he data into the folder data/processed_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c3a492-1f05-473a-a495-a7fbb416c0af",
   "metadata": {},
   "source": [
    "### 1) Step 1:Set to NaN all the in-cloud values to avoid cross-contamination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e0627c42-1e6f-4b17-a184-4e219f8bbaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_shallow(data_LW_down, clouds):\n",
    "    \"\"\"\n",
    "    This function sets to NaN all the values of LW_down in clouds to avoid cross-contamination.\n",
    "    \"\"\"\n",
    "\n",
    "    # Ensure data_LW_down uses Dask and chunk by a reasonable size\n",
    "    data_LW_down = data_LW_down.chunk({'time': 'auto'})\n",
    "\n",
    "    # Convert clouds to a Dask array to facilitate vectorized operations\n",
    "    clouds_array = da.from_array(clouds, chunks=(1000, 2))  # Adjust chunk size as needed\n",
    "\n",
    "    # Extract start and end times\n",
    "    start_times = xr.DataArray(clouds_array[:, 0], dims='cloud')\n",
    "    end_times = xr.DataArray(clouds_array[:, 1], dims='cloud')\n",
    "\n",
    "    # Create the initial combined mask using Dask operations\n",
    "    combined_mask = da.zeros(len(data_LW_down.time), dtype=bool)\n",
    "\n",
    "    for start_time, end_time in zip(start_times.values, end_times.values):\n",
    "        start_time = np.datetime64(start_time)\n",
    "        end_time = np.datetime64(end_time)\n",
    "        mask = (data_LW_down.time >= start_time) & (data_LW_down.time <= end_time)\n",
    "        combined_mask = combined_mask | mask.data\n",
    "\n",
    "    # Invert the combined mask to keep data outside cloudy periods\n",
    "    cloudy_mask = ~combined_mask\n",
    "\n",
    "    # Apply the combined cloudy mask\n",
    "    data_LW_down_masked = data_LW_down.where(cloudy_mask, np.nan)\n",
    "\n",
    "    # Compute the result with a progress bar\n",
    "    with ProgressBar():\n",
    "        result = data_LW_down_masked.compute()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22814a16-d360-4472-8357-5c6e057cccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#this code takes a long time to process. The result are already stoed in 'LW_down_2020_tot.nc'\n",
    "LW_down_NaN = process_shallow(LW_down, clouds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff56735-ff4d-4be9-8e1d-6752ee8e6f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "LW_down_NaN_xarray = xr.Dataset({'LWdown_diffuse': LW_down_NaN})\n",
    "LW_down_NaN_xarray.to_netcdf('./../data/processed_data/LW_down_2020_tot.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8e71f-6db6-471d-b582-96ca1eb6aa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "LW_down = LW_down_NaN_xarray['LWdown_diffuse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "517224a5-38e4-440c-b4f0-18a63c7d6b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LW_down = xr.open_dataset('./../data/processed_data/LW_down_2020_tot.nc')['LWdown_diffuse']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7317c94-0c04-46cf-9e08-184beb699584",
   "metadata": {},
   "source": [
    "### 2) Step 2: Separate the signal into single clouds arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "474072cc-1294-437e-a20c-6c05016da3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lw_down_clouds(LW_down, clouds):\n",
    "\n",
    "    \"\"\"\n",
    "    This function separates the LW_down measurements into single arrays for each cloud. This results in a main xarray that contains LW_down \n",
    "    measurements 60min before and after each cloud. There is a dimension for time and cloud, but also coordinates with each cloud characetiristics\n",
    "    for each cloud's array\n",
    "    \"\"\"\n",
    "    \n",
    "    half_t_cloud = 5 #half time/size of the cloud (clouds are rescaled to 10min)\n",
    "    delta_time = np.timedelta64(60, 'm') #time we look before and after the cloud \n",
    "    \n",
    "    # Initialize an empty list to store processed data for each cloud\n",
    "    processed_data = {}\n",
    "    cloud_length = []\n",
    "    cloud_bottom = []\n",
    "    cloud_top = []\n",
    "    cloud_mid = []\n",
    "    cloud_start_time = []\n",
    "    cloud_end_time = []\n",
    "\n",
    "    #iteration over each cloud\n",
    "    for cloud_index, cloud_times in enumerate(clouds):\n",
    "        \n",
    "        # Extract start and end time for the current cloud\n",
    "        start_time = np.datetime64(cloud_times[0])\n",
    "        end_time = np.datetime64(cloud_times[1])\n",
    "        \n",
    "        \n",
    "        # Extract data for the time window before the cloud\n",
    "        timedelta_before = start_time - delta_time\n",
    "        extracted_data_before = LW_down.sel(time=slice(timedelta_before, start_time))\n",
    "        extracted_data_before = extracted_data_before.assign_coords(time=extracted_data_before.time.values[::-1])\n",
    "        #we rescale the time such that it starts at 0 at the middle of the cloud\n",
    "        extracted_data_before['time'] = np.round(-np.abs((extracted_data_before.time - timedelta_before) / np.timedelta64(1, 's')) - half_t_cloud * 60 - 1)\n",
    "        \n",
    "        \n",
    "        # Extract data for the time window after the cloud\n",
    "        timedelta_after = end_time + delta_time\n",
    "        extracted_data_after = LW_down.sel(time=slice(end_time, timedelta_after))\n",
    "        #similar as before\n",
    "        extracted_data_after['time'] = np.round(np.abs((extracted_data_after.time - end_time) / np.timedelta64(1, 's')) + half_t_cloud * 60 + 1)\n",
    "\n",
    "        \n",
    "        # Extract data for the time window during the cloud\n",
    "        extracted_data_during = LW_down.sel(time=slice(start_time, end_time))\n",
    "        \n",
    "        if len(extracted_data_during) == 0:\n",
    "            continue\n",
    "\n",
    "        rescaled_time_during = np.abs((extracted_data_during.time.values - start_time) / np.timedelta64(1, 's'))\n",
    "        extracted_data_during['time'] = rescaled_time_during\n",
    "\n",
    "        #rescaling of the cloud such that each cloud has the same duration and we can compare different cloud lengths between them\n",
    "        min_time = np.nanmin(extracted_data_during['time'])\n",
    "        max_time = np.nanmax(extracted_data_during['time'])\n",
    "        rescaled_time = ((extracted_data_during['time'] - min_time) / (max_time - min_time)) * 600 - half_t_cloud * 60\n",
    "        \n",
    "        extracted_data_during['time'] = rescaled_time\n",
    "        \n",
    "        new_time = np.arange(-300, 301, 1)\n",
    "        extracted_data_during_unique = extracted_data_during.drop_duplicates(dim='time')\n",
    "        \n",
    "        extracted_data_during_unique = extracted_data_during_unique.sortby('time')\n",
    "        \n",
    "        time_diff = np.diff(extracted_data_during_unique['time'])\n",
    "        is_monotonic = (time_diff > 0).all() or (time_diff < 0).all()\n",
    "\n",
    "        # If the time coordinate is not strictly monotonic, sort the DataArray along the time dimension\n",
    "        if not is_monotonic:\n",
    "            continue\n",
    "\n",
    "        #for each cloud, append the data of LW_down, as well as the cloud's characteristics into a list called processed_data\n",
    "        try:\n",
    "            extracted_data_during_interpolated = extracted_data_during_unique.interp(time=new_time)\n",
    "\n",
    "            if ((extracted_data_during_interpolated < 10).any() or (extracted_data_before < 10).any() or (extracted_data_after < 10).any()):\n",
    "                continue\n",
    "        \n",
    "            cloud_length.append(clouds[cloud_index][2])\n",
    "            cloud_bottom.append(clouds[cloud_index][3])\n",
    "            cloud_top.append(clouds[cloud_index][4])\n",
    "            cloud_mid.append(clouds[cloud_index][5])\n",
    "            cloud_start_time.append(clouds[cloud_index][0])\n",
    "            cloud_end_time.append(clouds[cloud_index][1])\n",
    "            processed_data[cloud_index] = xr.concat([extracted_data_before, extracted_data_during_interpolated, extracted_data_after], dim='time')\n",
    "        except:\n",
    "            #print('ouch')\n",
    "            # Skip the current iteration if interpolation fails\n",
    "            continue\n",
    "    \n",
    "    # Concatenate the processed data along a new dimension 'cloud'\n",
    "    processed_data_combined = xr.concat([data.drop_duplicates('time') for data in processed_data.values()], dim='cloud')\n",
    "    processed_data_combined.coords['cloud_length'] = ('cloud', cloud_length)\n",
    "    processed_data_combined.coords['cloud_bottom'] = ('cloud', cloud_bottom)\n",
    "    processed_data_combined.coords['cloud_mid'] = ('cloud', cloud_mid)\n",
    "    processed_data_combined.coords['cloud_top'] = ('cloud', cloud_top)\n",
    "    processed_data_combined.coords['cloud_start_time'] = ('cloud', cloud_start_time)\n",
    "    processed_data_combined.coords['cloud_end_time'] = ('cloud', cloud_end_time)\n",
    "    #processed_data_combined = xr.concat([data for data in processed_data.values()], dim='cloud')\n",
    "    return processed_data_combined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7395474-0565-4cd3-8cd4-da42c02a54c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 57s, sys: 12.4 s, total: 10min 9s\n",
      "Wall time: 10min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "LW_down_clouds = lw_down_clouds(LW_down, clouds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ebf789e-84c3-4602-b514-a6338e19e7b1",
   "metadata": {},
   "source": [
    "### 3) Step 3: Convert the time coordinate into a distance coordinate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a41a44e-7dcb-4453-8a71-a5f171808c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lw_down_clouds_dist_approx(LW_down_clouds, windspeed_avg):\n",
    "    \"\"\"\n",
    "    Converts the time dimension into distance using the average windspeed. This approximation is justified since the focus is not on a few meters of \n",
    "    precision. This average windspeed is enough.\n",
    "    \"\"\"\n",
    "    # Calculate distance from cloud based on time and windspeed\n",
    "    distance_from_cloud = LW_down_clouds.time * windspeed_avg.item()\n",
    "    \n",
    "    # Rename the 'time' dimension to 'distance_from_cloud'\n",
    "    LW_down_clouds = LW_down_clouds.rename({'time': 'distance_from_cloud'})\n",
    "    \n",
    "    # Create a new coordinate 'distance' using the calculated distances\n",
    "    LW_down_clouds.coords['distance'] = ('distance_from_cloud', distance_from_cloud.data)\n",
    "    \n",
    "    # Replace the existing 'time' coordinate with the new 'distance' coordinate\n",
    "    LW_down_clouds = LW_down_clouds.swap_dims({'distance_from_cloud': 'distance'})\n",
    "    \n",
    "    # Add units attribute to the distance coordinate\n",
    "    LW_down_clouds.coords['distance'].attrs['units'] = 'm'\n",
    "    \n",
    "    return LW_down_clouds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6ee56c24-5576-451e-8d59-34f7e77d0db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.26 ms, sys: 6 µs, total: 1.27 ms\n",
      "Wall time: 1.07 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "LW_down_clouds_dist =  lw_down_clouds_dist_approx(LW_down_clouds, windspeed_avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e2fd2f-1d84-407a-83d6-eab732f1ae52",
   "metadata": {},
   "source": [
    "### 4) Step 4: save the xarray in processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "09d85f20-242c-4ffe-b16f-6381aadf0340",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.Dataset({'LWdown_diffuse': LW_down_clouds_dist})\n",
    "ds.to_netcdf('./../LW_down_clouds_2020_tot.nc')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "1 Python 3 (based on the module python3/2023.01)",
   "language": "python",
   "name": "python3_2023_01"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
